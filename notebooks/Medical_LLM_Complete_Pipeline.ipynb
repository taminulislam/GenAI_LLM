{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Medical LLM Training Pipeline\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Environment Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: c:\\Users\\Siu856569517\\Taminul\\GenAI_LLM\n",
            "Source path: c:\\Users\\Siu856569517\\Taminul\\GenAI_LLM\\src\n",
            "Python path updated\n",
            "Libraries imported successfully!\n",
            "Working directory: c:\\Users\\Siu856569517\\Taminul\\GenAI_LLM\\notebooks\n",
            "Python version: 3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]\n",
            "CUDA available: NVIDIA GeForce RTX 3090\n",
            "GPU Memory: 24.0GB\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import json\n",
        "import torch\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add project root to Python path\n",
        "project_root = Path.cwd()\n",
        "if project_root.name == \"notebooks\":\n",
        "    project_root = project_root.parent\n",
        "\n",
        "src_path = project_root / \"src\"\n",
        "if str(src_path) not in sys.path:\n",
        "    sys.path.insert(0, str(src_path))\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.insert(0, str(project_root))\n",
        "\n",
        "print(f\"Project root: {project_root}\")\n",
        "print(f\"Source path: {src_path}\")\n",
        "print(f\"Python path updated\")\n",
        "\n",
        "from config import config, MedicalLLMConfig\n",
        "from model_setup import ModelManager, get_model_memory_usage\n",
        "from data_loader import MedicalDataLoader\n",
        "from trainer import MedicalLLMTrainer, setup_training_environment\n",
        "from evaluator import MedicalLLMEvaluator\n",
        "\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(f\"Working directory: {Path.cwd()}\")\n",
        "print(f\"Python version: {sys.version}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA available: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")\n",
        "else:\n",
        "    print(\"CUDA not available - training will be slow\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:trainer:Setting up Medical LLM Training Environment...\n",
            "INFO:trainer:✅ CUDA Device: NVIDIA GeForce RTX 3090 (24.0GB)\n",
            "INFO:trainer:✅ All required packages imported successfully\n",
            "INFO:trainer:✅ Training environment ready!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting up Medical LLM Training Environment...\n",
            "Environment setup completed successfully!\n",
            "\n",
            "Current Configuration:\n",
            "==================================================\n",
            "Base Model: microsoft/DialoGPT-small\n",
            "Training Epochs: 2\n",
            "Batch Size: 2\n",
            "Learning Rate: 0.0002\n",
            "LoRA Rank (r): 32\n",
            "LoRA Alpha: 16\n",
            "Max Sequence Length: 512\n",
            "Use 4-bit Quantization: True\n",
            "Use FP16: True\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"Setting up Medical LLM Training Environment...\")\n",
        "environment_ready = setup_training_environment()\n",
        "\n",
        "if environment_ready:\n",
        "    print(\"Environment setup completed successfully!\")\n",
        "else:\n",
        "    print(\"Environment setup failed!\")\n",
        "    \n",
        "print(\"\\nCurrent Configuration:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Base Model: {config.model.base_model_name}\")\n",
        "print(f\"Training Epochs: {config.training.num_train_epochs}\")\n",
        "print(f\"Batch Size: {config.training.per_device_train_batch_size}\")\n",
        "print(f\"Learning Rate: {config.training.learning_rate}\")\n",
        "print(f\"LoRA Rank (r): {config.lora.r}\")\n",
        "print(f\"LoRA Alpha: {config.lora.lora_alpha}\")\n",
        "print(f\"Max Sequence Length: {config.training.max_seq_length}\")\n",
        "print(f\"Use 4-bit Quantization: {config.model.load_in_4bit}\")\n",
        "print(f\"Use FP16: {config.training.fp16}\")\n",
        "print(\"=\" * 50)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Dataset Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:data_loader:Loading real medical datasets...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading and preparing medical datasets...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:data_loader:📚 Dataset loaded with 10000 samples\n",
            "INFO:data_loader:🔄 Preprocessing dataset...\n",
            "INFO:data_loader:✅ Dataset preprocessing completed\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset prepared successfully!\n",
            "Total samples: 10000\n",
            "Sample format: ['instruction', 'input', 'output', '__index_level_0__', 'text', 'prompt', 'completion']\n",
            "\n",
            "Sample Data Examples:\n",
            "============================================================\n",
            "Sample 1:\n",
            "Instruction: If you are a doctor, please answer the medical questions based on the patient's description.\n",
            "Input: hi. im a home health aide and i have a client with scoliosis in the back and kidney dis...\n",
            "----------------------------------------\n",
            "Sample 2:\n",
            "Instruction: Please summerize the given abstract to a title\n",
            "Input: RATIONALE: The COVID-19 pandemic struck an immunologically naïve, globally interconnected population. In the face of a new infectious...\n",
            "----------------------------------------\n",
            "Sample 3:\n",
            "Instruction: Please summerize the given abstract to a title\n",
            "Input: Objectives: To investigate the experience of playing the harmonica for individuals with COPD. Methods: A qualitative, phenomenologica...\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "data_loader = MedicalDataLoader()\n",
        "\n",
        "print(\"Loading and preparing medical datasets...\")\n",
        "\n",
        "config.data.use_dummy_data = False\n",
        "\n",
        "data_loader.load_medical_dataset()\n",
        "data_loader.preprocess_dataset()\n",
        "\n",
        "print(\"Dataset prepared successfully!\")\n",
        "print(f\"Total samples: {len(data_loader.processed_dataset)}\")\n",
        "print(f\"Sample format: {list(data_loader.processed_dataset.features.keys())}\")\n",
        "\n",
        "print(\"\\nSample Data Examples:\")\n",
        "print(\"=\" * 60)\n",
        "for i in range(min(3, len(data_loader.processed_dataset))):\n",
        "    sample = data_loader.processed_dataset[i]\n",
        "    text = sample['text'][:200] + \"...\" if len(sample['text']) > 200 else sample['text']\n",
        "    print(f\"Sample {i+1}:\")\n",
        "    print(f\"{text}\")\n",
        "    print(\"-\" * 40)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Model Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:model_setup:Loading model: microsoft/DialoGPT-small\n",
            "INFO:model_setup:Set pad_token to eos_token\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting up model and tokenizer...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
            "INFO:model_setup:✅ Model loaded successfully!\n",
            "INFO:model_setup:Setting up LoRA configuration...\n",
            "INFO:model_setup:📈 Trainable parameters: 129,158,400\n",
            "INFO:model_setup:🔒 Total parameters: 129,158,400\n",
            "INFO:model_setup:📊 Trainable %: 100.00%\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuring LoRA adapters...\n",
            "\n",
            "Model setup completed!\n",
            "Base Model: microsoft/DialoGPT-small\n",
            "Model Type: PeftModelForCausalLM\n",
            "Tokenizer Vocab Size: 50257\n",
            "\n",
            "LoRA Configuration:\n",
            "Rank (r): 32\n",
            "Alpha: 16\n",
            "Dropout: 0.1\n",
            "Target Modules: ['c_attn', 'c_proj', 'c_fc']\n",
            "\n",
            "Parameter Efficiency:\n",
            "Total Parameters: 86,691,072\n",
            "Trainable Parameters: 4,718,592\n",
            "Trainable %: 5.44%\n",
            "\n",
            "GPU Memory Usage:\n",
            "Allocated: 0.48 GB\n",
            "Total: 24.0 GB\n",
            "Utilization: 3.2%\n"
          ]
        }
      ],
      "source": [
        "model_manager = ModelManager(config)\n",
        "\n",
        "print(\"Setting up model and tokenizer...\")\n",
        "model_manager.setup_model_and_tokenizer()\n",
        "\n",
        "print(\"Configuring LoRA adapters...\")\n",
        "model_manager.setup_lora_model()\n",
        "\n",
        "model_info = model_manager.get_model_info()\n",
        "print(\"\\nModel setup completed!\")\n",
        "print(f\"Base Model: {config.model.base_model_name}\")\n",
        "print(f\"Model Type: {type(model_manager.model).__name__}\")\n",
        "print(f\"Tokenizer Vocab Size: {len(model_manager.tokenizer)}\")\n",
        "\n",
        "print(\"\\nLoRA Configuration:\")\n",
        "print(f\"Rank (r): {config.lora.r}\")\n",
        "print(f\"Alpha: {config.lora.lora_alpha}\")\n",
        "print(f\"Dropout: {config.lora.lora_dropout}\")\n",
        "print(f\"Target Modules: {config.lora.target_modules}\")\n",
        "\n",
        "total_params = sum(p.numel() for p in model_manager.model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model_manager.model.parameters() if p.requires_grad)\n",
        "\n",
        "print(\"\\nParameter Efficiency:\")\n",
        "print(f\"Total Parameters: {total_params:,}\")\n",
        "print(f\"Trainable Parameters: {trainable_params:,}\")\n",
        "print(f\"Trainable %: {100 * trainable_params / total_params:.2f}%\")\n",
        "\n",
        "memory_usage = get_model_memory_usage()\n",
        "print(f\"\\nGPU Memory Usage:\")\n",
        "if \"error\" not in memory_usage:\n",
        "    print(f\"Allocated: {memory_usage['allocated_gb']} GB\")\n",
        "    print(f\"Total: {memory_usage['total_gb']} GB\")\n",
        "    print(f\"Utilization: {memory_usage['utilization_percent']}%\")\n",
        "else:\n",
        "    print(f\"Memory info: {memory_usage['error']}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:trainer:Starting Medical LLM Training Pipeline...\n",
            "INFO:trainer:Training arguments configured for output: ..\\experiments\\notebook_training_20250723_094231\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Medical LLM Training...\n",
            "Training Configuration:\n",
            "Epochs: 5\n",
            "Batch Size: 2\n",
            "Learning Rate: 0.0002\n",
            "\n",
            "Experiment Directory: ..\\experiments\\notebook_training_20250723_094231\n",
            "\n",
            "Training in progress...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c6661d3bc93b417a9131d275b1c295a3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Adding EOS to train dataset:   0%|          | 0/10000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b3e2d6f560d641dab3070271c570b8b9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing train dataset:   0%|          | 0/10000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (5145 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dd18c73410ea43c69ec759d7f3d37412",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Truncating train dataset:   0%|          | 0/10000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "INFO:trainer:SFTTrainer configured successfully\n",
            "wandb: Currently logged in as: taminul to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Siu856569517\\Taminul\\GenAI_LLM\\notebooks\\wandb\\run-20250723_094244-ubrueoyk</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/taminul/medical-llm-finetuning/runs/ubrueoyk' target=\"_blank\">medical-llm-20250723_094231</a></strong> to <a href='https://wandb.ai/taminul/medical-llm-finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/taminul/medical-llm-finetuning' target=\"_blank\">https://wandb.ai/taminul/medical-llm-finetuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/taminul/medical-llm-finetuning/runs/ubrueoyk' target=\"_blank\">https://wandb.ai/taminul/medical-llm-finetuning/runs/ubrueoyk</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:trainer:Starting training...\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6250/6250 1:05:36, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>10.430500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>8.252200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>5.854700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>5.288800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>4.953400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>4.762200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>4.540400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>4.496100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>4.245300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>4.390000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>4.272100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>4.239400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>4.131400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>4.146100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>4.054900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>4.047900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>4.016200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>4.044100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>4.077100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>3.950600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>4.070200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>3.936600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>3.988800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>3.878500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>3.836100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>3.958100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>3.934600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>3.854400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1450</td>\n",
              "      <td>3.808100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>3.870400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1550</td>\n",
              "      <td>3.904500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>3.813400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1650</td>\n",
              "      <td>3.850700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>3.826600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1750</td>\n",
              "      <td>3.818700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>3.763400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1850</td>\n",
              "      <td>3.798200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>3.752500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1950</td>\n",
              "      <td>3.670100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>3.767000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2050</td>\n",
              "      <td>3.718300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>3.826500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2150</td>\n",
              "      <td>3.713000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>3.734800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2250</td>\n",
              "      <td>3.830600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>3.740800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2350</td>\n",
              "      <td>3.737000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>3.764200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2450</td>\n",
              "      <td>3.725100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>3.655200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2550</td>\n",
              "      <td>3.701200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>3.730500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2650</td>\n",
              "      <td>3.556100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>3.764400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2750</td>\n",
              "      <td>3.696200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>3.676900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2850</td>\n",
              "      <td>3.679900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>3.582400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2950</td>\n",
              "      <td>3.634000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>3.656900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3050</td>\n",
              "      <td>3.668200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3100</td>\n",
              "      <td>3.646800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3150</td>\n",
              "      <td>3.658000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3200</td>\n",
              "      <td>3.545400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3250</td>\n",
              "      <td>3.638300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3300</td>\n",
              "      <td>3.598700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3350</td>\n",
              "      <td>3.565300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3400</td>\n",
              "      <td>3.619900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3450</td>\n",
              "      <td>3.696300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>3.638500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3550</td>\n",
              "      <td>3.640100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3600</td>\n",
              "      <td>3.610900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3650</td>\n",
              "      <td>3.639200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3700</td>\n",
              "      <td>3.692200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3750</td>\n",
              "      <td>3.624600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3800</td>\n",
              "      <td>3.515800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3850</td>\n",
              "      <td>3.612400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3900</td>\n",
              "      <td>3.561300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3950</td>\n",
              "      <td>3.606000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>3.507700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4050</td>\n",
              "      <td>3.618000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4100</td>\n",
              "      <td>3.651800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4150</td>\n",
              "      <td>3.643100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4200</td>\n",
              "      <td>3.573600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4250</td>\n",
              "      <td>3.591100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4300</td>\n",
              "      <td>3.466800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4350</td>\n",
              "      <td>3.543600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4400</td>\n",
              "      <td>3.547900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4450</td>\n",
              "      <td>3.586600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>3.634400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4550</td>\n",
              "      <td>3.527700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4600</td>\n",
              "      <td>3.501800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4650</td>\n",
              "      <td>3.491300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4700</td>\n",
              "      <td>3.511500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4750</td>\n",
              "      <td>3.542400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4800</td>\n",
              "      <td>3.527800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4850</td>\n",
              "      <td>3.511400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4900</td>\n",
              "      <td>3.449000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4950</td>\n",
              "      <td>3.613600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>3.579600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5050</td>\n",
              "      <td>3.562300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5100</td>\n",
              "      <td>3.490400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5150</td>\n",
              "      <td>3.577900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5200</td>\n",
              "      <td>3.611900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5250</td>\n",
              "      <td>3.506600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5300</td>\n",
              "      <td>3.476900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5350</td>\n",
              "      <td>3.567100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5400</td>\n",
              "      <td>3.516500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5450</td>\n",
              "      <td>3.464600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>3.472400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5550</td>\n",
              "      <td>3.491500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5600</td>\n",
              "      <td>3.493600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5650</td>\n",
              "      <td>3.470700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5700</td>\n",
              "      <td>3.553900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5750</td>\n",
              "      <td>3.488300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5800</td>\n",
              "      <td>3.469400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5850</td>\n",
              "      <td>3.464900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5900</td>\n",
              "      <td>3.506800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5950</td>\n",
              "      <td>3.505700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>3.465300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6050</td>\n",
              "      <td>3.567600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6100</td>\n",
              "      <td>3.556900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6150</td>\n",
              "      <td>3.549500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6200</td>\n",
              "      <td>3.542300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6250</td>\n",
              "      <td>3.525500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wandb: WARNING The get_url method is deprecated and will be removed in a future release. Please use `run.url` instead.\n",
            "INFO:trainer:Saving trained model...\n",
            "INFO:model_setup:💾 Model saved to ..\\experiments\\notebook_training_20250723_094231\\final_model\n",
            "INFO:trainer:Training completed! Results saved to: ..\\experiments\\notebook_training_20250723_094231\n",
            "INFO:trainer:Final training loss: 3.8457\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training completed!\n",
            "Final Loss: 3.8457\n",
            "Training Steps: 6250\n",
            "Epochs Completed: 5\n",
            "Model Saved: ..\\experiments\\notebook_training_20250723_094231\\final_model\n",
            "\n",
            "Results saved to: ..\\experiments\\notebook_training_20250723_094231\n"
          ]
        }
      ],
      "source": [
        "config.training.num_train_epochs = 5\n",
        "config.training.logging_steps = 50\n",
        "\n",
        "print(\"Starting Medical LLM Training...\")\n",
        "print(f\"Training Configuration:\")\n",
        "print(f\"Epochs: {config.training.num_train_epochs}\")\n",
        "print(f\"Batch Size: {config.training.per_device_train_batch_size}\")\n",
        "print(f\"Learning Rate: {config.training.learning_rate}\")\n",
        "\n",
        "trainer = MedicalLLMTrainer(config)\n",
        "\n",
        "experiment_name = f\"notebook_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "experiment_dir = Path(\"../experiments\") / experiment_name\n",
        "experiment_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"\\nExperiment Directory: {experiment_dir}\")\n",
        "\n",
        "print(\"\\nTraining in progress...\")\n",
        "training_results = trainer.train(\n",
        "    model_manager=model_manager,\n",
        "    data_loader=data_loader,\n",
        "    output_dir=str(experiment_dir)\n",
        ")\n",
        "\n",
        "print(\"\\nTraining completed!\")\n",
        "print(f\"Final Loss: {training_results['train_loss']:.4f}\")\n",
        "print(f\"Training Steps: {training_results['train_steps']}\")\n",
        "print(f\"Epochs Completed: {training_results['epochs_trained']}\")\n",
        "print(f\"Model Saved: {training_results['final_model_path']}\")\n",
        "\n",
        "with open(experiment_dir / \"notebook_results.json\", 'w') as f:\n",
        "    json.dump(training_results, f, indent=2, default=str)\n",
        "\n",
        "print(f\"\\nResults saved to: {experiment_dir}\")\n",
        "\n",
        "final_model_path = training_results['final_model_path']\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting comprehensive model evaluation...\n",
            "Model to evaluate: ..\\experiments\\notebook_training_20250723_094231\\final_model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:evaluator:🚀 Starting Comprehensive Medical LLM Evaluation...\n",
            "INFO:evaluator:Loading model from: ..\\experiments\\notebook_training_20250723_094231\\final_model\n",
            "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
            "INFO:model_setup:✅ Trained model loaded from ..\\experiments\\notebook_training_20250723_094231\\final_model\n",
            "INFO:evaluator:Model ready for evaluation\n",
            "INFO:evaluator:Loading medical benchmark datasets...\n",
            "INFO:evaluator:Loading MedQA dataset...\n",
            "INFO:evaluator:✅ MedQA loaded: 500 samples\n",
            "INFO:evaluator:Loading PubMedQA dataset...\n",
            "WARNING:evaluator:⚠️ Could not load PubMedQA: Unknown split \"test\". Should be one of ['train'].\n",
            "INFO:evaluator:Benchmark datasets loaded: ['medqa']\n",
            "INFO:evaluator:Evaluating on medqa (500 samples)...\n",
            "Device set to use cuda:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=206) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=208) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=299) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=163) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=204) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=172) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=212) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=232) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=174) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=277) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "INFO:evaluator:Processed 10/50 questions\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=212) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=272) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=194) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=187) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=183) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=293) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=294) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=214) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=261) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "INFO:evaluator:Processed 20/50 questions\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=209) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=196) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=167) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=240) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=284) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=386) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=180) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=234) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=338) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=338) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "INFO:evaluator:Processed 30/50 questions\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=196) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=279) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=224) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=365) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=188) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=311) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=289) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=231) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=217) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=292) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "INFO:evaluator:Processed 40/50 questions\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=264) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=182) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=201) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=195) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=221) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=191) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=223) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=163) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=208) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "INFO:evaluator:Processed 50/50 questions\n",
            "INFO:evaluator:✅ medqa Evaluation Complete:\n",
            "INFO:evaluator:   Accuracy: 0.940 (47/50)\n",
            "INFO:evaluator:💾 Evaluation results saved to: evaluation\\evaluation_results_20250723_110100.json\n",
            "INFO:evaluator:🎉 Comprehensive Evaluation Complete!\n",
            "INFO:evaluator:📊 Overall Accuracy: 0.940 (47/50)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluation completed!\n",
            "\n",
            "Evaluation Summary:\n",
            "==================================================\n",
            "Overall Accuracy: 0.940\n",
            "Total Questions: 50\n",
            "Correct Answers: 47\n",
            "Benchmarks Evaluated: 1\n",
            "\n",
            "Benchmark Performance:\n",
            "==================================================\n",
            "medqa:\n",
            "   Accuracy: 0.940\n",
            "   Questions: 50\n",
            "   Correct: 47\n",
            "\n",
            "Detailed Evaluation Report:\n",
            "============================================================\n",
            "============================================================\n",
            "MEDICAL LLM EVALUATION REPORT\n",
            "============================================================\n",
            "Model: microsoft/DialoGPT-small\n",
            "Evaluation Date: 2025-07-23T10:57:57.036633\n",
            "\n",
            "OVERALL RESULTS:\n",
            "  Overall Accuracy: 0.940\n",
            "  Total Questions: 50\n",
            "  Correct Answers: 47\n",
            "  Benchmarks Evaluated: 1\n",
            "\n",
            "BENCHMARK DETAILS:\n",
            "  medqa: 0.940 (47/50)\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "evaluator = MedicalLLMEvaluator(config)\n",
        "\n",
        "print(\"Starting comprehensive model evaluation...\")\n",
        "print(f\"Model to evaluate: {final_model_path}\")\n",
        "\n",
        "evaluation_results = evaluator.run_comprehensive_evaluation(final_model_path)\n",
        "\n",
        "print(\"\\nEvaluation completed!\")\n",
        "\n",
        "summary = evaluation_results.get('summary', {})\n",
        "print(\"\\nEvaluation Summary:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Overall Accuracy: {summary.get('overall_accuracy', 0):.3f}\")\n",
        "print(f\"Total Questions: {summary.get('total_questions', 0)}\")\n",
        "print(f\"Correct Answers: {summary.get('total_correct', 0)}\")\n",
        "print(f\"Benchmarks Evaluated: {summary.get('benchmarks_evaluated', 0)}\")\n",
        "\n",
        "benchmark_results = evaluation_results.get('benchmark_results', {})\n",
        "print(\"\\nBenchmark Performance:\")\n",
        "print(\"=\" * 50)\n",
        "for benchmark_name, results in benchmark_results.items():\n",
        "    if 'error' not in results:\n",
        "        accuracy = results.get('accuracy', 0)\n",
        "        total_q = results.get('total_questions', 0)\n",
        "        correct = results.get('correct_answers', 0)\n",
        "        print(f\"{benchmark_name}:\")\n",
        "        print(f\"   Accuracy: {accuracy:.3f}\")\n",
        "        print(f\"   Questions: {total_q}\")\n",
        "        print(f\"   Correct: {correct}\")\n",
        "    else:\n",
        "        print(f\"{benchmark_name}: ERROR - {results['error']}\")\n",
        "\n",
        "report = evaluator.create_evaluation_report(evaluation_results)\n",
        "print(\"\\nDetailed Evaluation Report:\")\n",
        "print(\"=\" * 60)\n",
        "print(report)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Results Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summary Statistics:\n",
            "========================================\n",
            "                   Metric      Value\n",
            "         Total Parameters 86,691,072\n",
            "     Trainable Parameters  4,718,592\n",
            " Parameter Efficiency (%)      5.44%\n",
            "         Overall Accuracy      0.940\n",
            "Total Questions Evaluated         50\n",
            "          Correct Answers         47\n",
            "\n",
            "Model Performance Level: Excellent\n",
            "Accuracy: 0.940\n",
            "\n",
            "Improvement vs Random Baseline: +0.690\n",
            "\n",
            "Experiment completed successfully!\n",
            "Model saved at: ..\\experiments\\notebook_training_20250723_094231\\final_model\n"
          ]
        }
      ],
      "source": [
        "stats_data = {\n",
        "    'Metric': [\n",
        "        'Total Parameters',\n",
        "        'Trainable Parameters', \n",
        "        'Parameter Efficiency (%)',\n",
        "        'Overall Accuracy',\n",
        "        'Total Questions Evaluated',\n",
        "        'Correct Answers'\n",
        "    ],\n",
        "    'Value': [\n",
        "        f\"{total_params:,}\",\n",
        "        f\"{trainable_params:,}\",\n",
        "        f\"{100 * trainable_params / total_params:.2f}%\",\n",
        "        f\"{summary.get('overall_accuracy', 0):.3f}\",\n",
        "        f\"{summary.get('total_questions', 0)}\",\n",
        "        f\"{summary.get('total_correct', 0)}\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "stats_df = pd.DataFrame(stats_data)\n",
        "print(\"Summary Statistics:\")\n",
        "print(\"=\" * 40)\n",
        "print(stats_df.to_string(index=False))\n",
        "\n",
        "overall_accuracy = summary.get('overall_accuracy', 0)\n",
        "if overall_accuracy >= 0.8:\n",
        "    performance_level = \"Excellent\"\n",
        "elif overall_accuracy >= 0.6:\n",
        "    performance_level = \"Good\"\n",
        "elif overall_accuracy >= 0.4:\n",
        "    performance_level = \"Fair\"\n",
        "else:\n",
        "    performance_level = \"Poor\"\n",
        "\n",
        "print(f\"\\nModel Performance Level: {performance_level}\")\n",
        "print(f\"Accuracy: {overall_accuracy:.3f}\")\n",
        "\n",
        "improvement_over_random = overall_accuracy - 0.25\n",
        "print(f\"\\nImprovement vs Random Baseline: +{improvement_over_random:.3f}\")\n",
        "\n",
        "print(f\"\\nExperiment completed successfully!\")\n",
        "print(f\"Model saved at: {final_model_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "medical_llm_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
